{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using preprocessed annotations for comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook imports manual annotations and compares them with the automated metrics, by individual algorithm. <br>\n",
    "Their agreement is analyzed by various metrics, including sensitivity, specificity, PPV, NPV, Rand Accuracy. <br>\n",
    "These are calculated at the bottom of this script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAUTION:\n",
    "    This method requires the test_code and test month to be present for each gold-standard entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up local environment\n",
    "from cw_package.setup_cw_env import *\n",
    "from pylab import *\n",
    "from cw_package import prDF\n",
    "from au_package import assess_agreement, prep_Au_standard, standardize_flags\n",
    "import pickle\n",
    "#import re\n",
    "jacks_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name local variables\n",
    "coded_date = '2017_07_12allfeed_woI10'\n",
    "earliest_date= '2016_04_01'\n",
    "time_period= ['2016_4','2016_5','2016_6','2016_7','2016_8','2016_9']\n",
    "elevenkeys= ['CW_cerv','CW_card','CW_vitd','CW_bph','CW_lbp','CW_feed','CW_psyc','CW_dexa','CW_narc',\n",
    "             'CW_nonpreop','CW_catpreop']\n",
    "stdflag_string= 'as_annotated'\n",
    "\n",
    "GEMdicts = ['10to10','claimto9best', 'claimto9reimb', 'refto10best','refto10gems']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import semi-preprocessed annotations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from the original annotations done for first CW version\n",
    "treasurechest_r = pd.read_pickle('./preprocessed/'+coded_date+'/pickled_treasurechest_r_'+coded_date+'.p')\n",
    "# Load annotations from the Seibert's annotations done for Feb 28th data CW version\n",
    "treasurechest_s1_r = pd.read_pickle('./preprocessed/'+coded_date+'/pickled_treasurechest_s1_r_'+coded_date+'.p')\n",
    "# Load annotations from the Seibert's annotations done for May 14 CW version\n",
    "treasurechest_s2_r = pd.read_pickle('./preprocessed/'+coded_date+'/pickled_treasurechest_s2_r_'+coded_date+'.p')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GEM implementation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compiled numerators and denominators from different GEM implementation analyses\n",
    "numerators_lt   = pd.read_pickle('./preprocessed/'+coded_date+'/pickled_compilednumerators_'+coded_date+'_long.p')\n",
    "denominators_lt = pd.read_pickle('./preprocessed/'+coded_date+'/pickled_compileddenominators_'+coded_date+'_long.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the numerators from the denominators in each GEM implementation\n",
    "Convert denominators to specifically \"Not-numerator\" cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuples, that are more appropriately identified as \"Not Numerators\"\n",
    "notnumerators_lt = [None]*len(numerators_lt)\n",
    "for x in range(len(numerators_lt)):\n",
    "    if denominators_lt[x][0][3:]!=numerators_lt[x][0][3:]:\n",
    "        raise ValueError('numerators and denominators have different GEMs or order to GEMs')\n",
    "    else:\n",
    "        num_MRNonly = numerators_lt[x][1]\n",
    "        num_MRNonly = num_MRNonly[num_MRNonly.Metric.isin(['CW_bph','CW_narc','CW_feed','CW_psyc','CW_lbp'])]\n",
    "        num_MRNonly = num_MRNonly[['MRN','TEST_DATE_month','Metric']]\n",
    "        num_MRNonly['bean']=1\n",
    "        \n",
    "        num_3merge = numerators_lt[x][1]\n",
    "        num_3merge = num_3merge[~num_3merge.Metric.isin(['CW_bph','CW_narc','CW_feed','CW_psyc','CW_lbp'])]\n",
    "        num_3merge = num_3merge[['MRN','TEST_CODE','TEST_DATE_a','TEST_DATE_month','Metric']]\n",
    "        num_3merge['bean']=1\n",
    "        \n",
    "        \n",
    "        den_MRNonly = denominators_lt[x][1]\n",
    "        den_MRNonly = den_MRNonly[den_MRNonly.Metric.isin(['CW_bph','CW_narc','CW_feed','CW_psyc','CW_lbp'])]\n",
    "        \n",
    "        den_3merge = denominators_lt[x][1]\n",
    "        den_3merge = den_3merge[~den_3merge.Metric.isin(['CW_bph','CW_narc','CW_feed','CW_psyc','CW_lbp'])]\n",
    "        \n",
    "        den_MRNonly = den_MRNonly.merge(num_MRNonly, on=['MRN','TEST_DATE_month','Metric'], how='left')\n",
    "        den_3merge  = den_3merge.merge(num_3merge, \n",
    "                                       on=['MRN','TEST_CODE','TEST_DATE_a','TEST_DATE_month','Metric'],\n",
    "                                       how='left')\n",
    "        \n",
    "        \n",
    "    \n",
    "        notnum = pd.concat([den_MRNonly, den_3merge])\n",
    "        notnum = notnum[notnum.bean!=1]\n",
    "        notnumerators_lt[x]= (denominators_lt[x][0][3:], notnum)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point,\n",
    "numerators_lt and notnumerators_lt are the analyses' results for assigning numerator status\n",
    "(rather than numerator and denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign standardized label\n",
    "for x in notnumerators_lt:\n",
    "    x[1]['Term per GEM eval']='Not_Numer'\n",
    "for x in numerators_lt:\n",
    "    x[1]['Term per GEM eval']='Numer'\n",
    "\n",
    "# concatenate them into single dataframes by Metric, w/in individual GEM implementations\n",
    "allterms_lt = [None]*len(numerators_lt)\n",
    "for x in range(len(numerators_lt)):\n",
    "    allterms_lt[x]=(numerators_lt[x][0][3:], pd.concat([notnumerators_lt[x][1], numerators_lt[x][1]]))\n",
    "    allterms_lt[x][1][allterms_lt[x][0]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail_lt = pd.concat([x[1].groupby(['Metric','Term per GEM eval']).count() for x in allterms_lt], axis=1)\n",
    "avail_lt = avail_lt[['_'+x for x in GEMdicts]]\n",
    "avail_lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, work with the gold standards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep the gold standards by standardizing the flags to match\n",
    "depends on stdflag_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treasurechest_r_std=treasurechest_r\n",
    "treasurechest_s1_r_std=treasurechest_s1_r\n",
    "treasurechest_s2_r_std=treasurechest_s2_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Take the numerators from the analyses that are in [treasurechest numerators, not numerators]\n",
    "+ See how many of them are marked as such in the treasurechest\n",
    "+ See how many are marked as not numerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in treasurechest_r_std:\n",
    "    #print(x['Metric'])\n",
    "    x['linked']=pd.concat([standardize_flags(x['Numerator'],stdflag_string),standardize_flags(x['Denominator'],stdflag_string)],axis=0)\n",
    "    # following line makes sure that incident_service counted once if it was in both numerator and denominator\n",
    "    x['linked_a']=x['linked'].groupby(['MRN','TEST_CODE','Term_assessed','TEST_DATE_month','Gold_Standard']).count()\n",
    "    x['linked_a']=x['linked_a'].reset_index()\n",
    "    x['linked_a']['Metric']=x['Metric']\n",
    "print('Annotation flags have been standardized, per [{}-keyword] setting'.format(stdflag_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in treasurechest_s1_r_std:\n",
    "    print(x['Metric'])\n",
    "    x['linked']= standardize_flags(x['Annotated'],stdflag_string)\n",
    "    x['linked_a']=x['linked'].groupby(['MRN','TEST_CODE','Term_assessed','TEST_DATE_month','Gold_Standard']).count()\n",
    "    x['linked_a']=x['linked_a'].reset_index()\n",
    "    x['linked_a']['Metric']=x['Metric']\n",
    "    x['linked_a']=x['linked_a'][x['linked_a'].Term_assessed.isin(['Numer','Not_Numer'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in treasurechest_s2_r_std:\n",
    "    print(x['Metric'])\n",
    "    x['linked']= standardize_flags(x['Annotated'],stdflag_string)\n",
    "    x['linked_a']=x['linked'].groupby(['MRN','TEST_CODE','Term_assessed','TEST_DATE_month','Gold_Standard']).count()\n",
    "    x['linked_a']=x['linked_a'].reset_index()\n",
    "    x['linked_a']['Metric']=x['Metric']\n",
    "    x['linked_a']=x['linked_a'][x['linked_a'].Term_assessed.isin(['Numer','Not_Numer'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlepotofAu_orig= pd.concat([x['linked_a'] for x in treasurechest_r_std])\n",
    "singlepotofAu_s1 = pd.concat([x['linked_a'] for x in treasurechest_s1_r_std])\n",
    "singlepotofAu_s2= pd.concat([x['linked_a'] for x in treasurechest_s2_r_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlepotofAu_s1.rename(columns={'TEST_DATE_a':'TEST_DATE'},inplace=True)\n",
    "singlepotofAu_s2.rename(columns={'TEST_DATE_a':'TEST_DATE'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link the annotation sets into single dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlepotofAu = pd.concat([singlepotofAu_orig,\n",
    "                              singlepotofAu_s1,\n",
    "                              singlepotofAu_s2])\n",
    "singlepotofAu= singlepotofAu[singlepotofAu['Term_assessed']!='See comment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlepotofAu=singlepotofAu.sort(['Metric','MRN','TEST_CODE','TEST_DATE_month','Term_assessed'], \n",
    "                   ascending=[True,True,True,True,False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleAu_MRNonly = singlepotofAu[singlepotofAu.Metric.isin(['CW_bph','CW_narc','CW_feed','CW_psyc','CW_lbp'])]\n",
    "singleAu_3merge  = singlepotofAu[~singlepotofAu.Metric.isin(['CW_bph','CW_narc','CW_feed','CW_psyc','CW_lbp'])]\n",
    "\n",
    "singleAu_MRNonly.drop_duplicates(subset=['Metric','MRN','TEST_DATE_month','Term_assessed'], inplace=True)\n",
    "singleAu_3merge.drop_duplicates(subset=['Metric','MRN','TEST_DATE_month','TEST_CODE','Term_assessed'], inplace=True)\n",
    "\n",
    "singlepotofAu = pd.concat([singleAu_MRNonly, singleAu_3merge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________\n",
    "# At this point,\n",
    "- (1) each GEM implementation has all numer/not_numer for each metric within single-unified dataframe.\n",
    "- (2) the annotations are in single dataframe.\n",
    "____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Take the notnumerators from the analyses, that are in [treasurechest numerators, not numerators]\n",
    "+ See how many of them are marked as such in the treasurechest\n",
    "+ See how many are marked as not numerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory for exporting results\n",
    "try:\n",
    "    os.mkdir('scrapdir/'+coded_date)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_results(singleGEMdf, GEM_name, unifiedAu_std, methodx, GEM_or_Au):\n",
    "    # add tags\n",
    "    Termcolname_a = 'Term per GEM eval' if GEM_or_Au=='GEM' else 'Term_assessed'\n",
    "    Termcolname_b = 'Term_assessed' if GEM_or_Au=='GEM' else 'Term per GEM eval'\n",
    "    if GEM_or_Au =='GEM':\n",
    "        # (1) split the df into two parts\n",
    "        used_MRNonly = singleGEMdf[singleGEMdf.Metric.isin(['CW_bph','CW_narc','CW_feed','CW_psyc','CW_lbp'])]\n",
    "        used_3merge  = singleGEMdf[~singleGEMdf.Metric.isin(['CW_bph','CW_narc','CW_feed','CW_psyc','CW_lbp'])]\n",
    "        \n",
    "        # break into not numerator and numerator. \n",
    "        used_dMRNonly = used_MRNonly[used_MRNonly[Termcolname_a]=='Not_Numer']\n",
    "        used_nMRNonly = used_MRNonly[used_MRNonly[Termcolname_a]!='Not_Numer']\n",
    "        # merge the metric data with annotation data\n",
    "        used_dMRNonly = used_dMRNonly.merge(unifiedAu_std, on=['Metric', 'MRN', 'TEST_DATE_month'],  how=methodx)\n",
    "        used_nMRNonly = used_nMRNonly.merge(unifiedAu_std, on=['Metric', 'MRN', 'TEST_CODE', 'TEST_DATE_month'], how=methodx)\n",
    "        # merge the metric data with annotation data (this works for denominators defined by test)\n",
    "        used_3merge = used_3merge.merge(unifiedAu_std, on=['Metric', 'MRN', 'TEST_CODE', 'TEST_DATE_month'], how=methodx)\n",
    "        # bring these three merged df's back into single df.\n",
    "        used = pd.concat([used_dMRNonly, used_nMRNonly, used_3merge])\n",
    "        \n",
    "        \n",
    "        # drop the rows without Au-standard correlate; \n",
    "        #      This means, only assess accuracy for cases identified and reviewed in manual annotations. \n",
    "        #      Therefore, cases missed by not being included in denominator were inappropriately lost,\n",
    "        #      and this error will not be detected by standard measures of sens/specificity/etc\n",
    "        GEM_asroot=used.dropna(subset=[Termcolname_b])[[GEM_name,'Metric','MRN','TEST_CODE','TEST_DATE_month',Termcolname_a,Termcolname_b]]\n",
    "        \n",
    "        # Actually check for the agreement between annotations and automated assignments\n",
    "        GEM_asroot.loc[GEM_asroot[Termcolname_a]==GEM_asroot[Termcolname_b],'Agree']=1\n",
    "        GEM_asroot.loc[GEM_asroot[Termcolname_a]!=GEM_asroot[Termcolname_b],'Agree']=0\n",
    "        ### deleted line: GEM_asroot['Agree']=np.where(GEM_asroot[Termcolname_a]==GEM_asroot[Termcolname_b], 1, 0)\n",
    "        GEM_asrootNN = GEM_asroot.loc[GEM_asroot[Termcolname_a]=='Not_Numer',:]\n",
    "        GEM_asrootN = GEM_asroot.loc[GEM_asroot[Termcolname_a]!='Not_Numer',:]\n",
    "        # determine true and false positive/negatives\n",
    "        GEM_asrootNN.loc[GEM_asrootNN['Agree']==1,'Agreement_eval']='TNot_Numer'\n",
    "        GEM_asrootNN.loc[GEM_asrootNN['Agree']!=1,'Agreement_eval']='FNot_Numer'\n",
    "        ### deleted line: GEM_asrootNN.loc[:,'Agreement_eval']=np.where(GEM_asrootNN['Agree']==1,'TNot_Numer','FNot_Numer')\n",
    "        GEM_asrootN.loc[GEM_asrootN['Agree']==1,'Agreement_eval']='TNumer'\n",
    "        GEM_asrootN.loc[GEM_asrootN['Agree']!=1,'Agreement_eval']='FNumer'\n",
    "        ### deleted line: GEM_asrootN.loc[:,'Agreement_eval']=np.where(GEM_asrootN['Agree']==1,'TNumer','FNumer')\n",
    "        \n",
    "        # bring results back together\n",
    "        GEM_asroot = pd.concat([GEM_asrootNN,GEM_asrootN])\n",
    "        \n",
    "        \n",
    "        intermed = GEM_asroot.groupby(['Metric','Agreement_eval'])['Agree'].count()\n",
    "        intermed = intermed.reset_index()\n",
    "        intermed['GEM']= GEM_name\n",
    "        #idiosyncratic line: splice the string in 'Agreement_eval to capture the u in Numerator\n",
    "        intermed.loc[:,'GEM result']=intermed['Agreement_eval'].apply(lambda x: '+ Numer' if x[2]=='u' else '- Not Numer')\n",
    "        intermed.loc[intermed['GEM result']=='+ Numer','Annotated Standard']=intermed['Agreement_eval'].apply(lambda x: '- Not Numer' if x[0]=='F' else '+ Numer')\n",
    "        intermed.loc[intermed['GEM result']=='- Not Numer','Annotated Standard']=intermed['Agreement_eval'].apply(lambda x: '+ Numer' if x[0]=='F' else '- Not Numer')\n",
    "        return (GEM_name, GEM_asroot, intermed)\n",
    "    else:\n",
    "        print('processing')\n",
    "        Au_asroot= unifiedAu_std.merge(singleGEMdf, on=['Metric', 'MRN', 'TEST_CODE', 'TEST_DATE_month'], how=methodx)\n",
    "        Au_asroot.loc[Au_asroot[Termcolname_b].isnull(),'omitted']=1\n",
    "        intermed = Au_asroot.groupby(['Metric','omitted'])['Metric'].count()\n",
    "        intermed = intermed.reset_index()\n",
    "        intermed['GEM']= GEM_name\n",
    "        return (GEM_name, Au_asroot, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tester = eval_results(allterms_lt[0][1],'_claimto9best', singlepotofAu, 'inner','GEM')\n",
    "compared = []\n",
    "for x in allterms_lt:\n",
    "    compared.append(eval_results(x[1], x[0], singlepotofAu,'inner','GEM'))\n",
    "unifiedcompared=pd.concat([x[2] for x in compared])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ For inspecting individual records] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jx=allterms_lt[2][1]\n",
    "jx=jx[jx.MRN=='<insert MRN here>'] # caution --- this is a place where PHI could be inadvertently stored.\n",
    "jx[['Metric','MRN','TEST_CODE','TEST_DATE','TEST_DATE_month','Term per GEM eval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=compared[0][1]\n",
    "j=j[j.Metric=='CW_card']\n",
    "liv=j[j.Agree==0]\n",
    "liv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempAu=singlepotofAu\n",
    "tempAu['bean']=1\n",
    "tempAu=tempAu.groupby(['Metric','Term_assessed'])['bean'].count()\n",
    "tempAu=tempAu.reset_index()\n",
    "tA=tempAu.pivot_table('bean','Metric','Term_assessed')\n",
    "print('These are the total available annotations within time period')\n",
    "tA[['Numer', 'Not_Numer']].to_csv('./scrapdir/'+coded_date+'/totalannotationsavail_'+coded_date+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect and export 2x2's for each metric, by GEM (Numerator v. not-numerator assignments)\n",
    "out=unifiedcompared.pivot_table('Agree', ['Metric','GEM','GEM result'],['Annotated Standard'])\n",
    "out.to_csv('./scrapdir/'+coded_date+'/totalpivot_'+coded_date+'.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating sensitivities and specificities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unifiedcompared=unifiedcompared.fillna(0)\n",
    "sens_spec = unifiedcompared.pivot_table('Agree', ['Metric','GEM','Annotated Standard'],['GEM result']).fillna(0)\n",
    "sens_spec.reset_index(inplace=True)\n",
    "sens_spec.loc[sens_spec['Annotated Standard']=='+ Numer','Sensitivity']=sens_spec['+ Numer'].divide((sens_spec['+ Numer']+ sens_spec['- Not Numer']), axis=0, fill_value=0)#.apply(lambda x: 100*np.round(x,3))\n",
    "sens_spec.loc[sens_spec['Annotated Standard']=='- Not Numer','Specificity']=sens_spec['- Not Numer'].divide((sens_spec['+ Numer']+ sens_spec['- Not Numer']), axis=0, fill_value=0)#.apply(lambda x: 100*np.round(x,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=unifiedcompared.groupby(['Metric','GEM']).count()\n",
    "stem.reset_index(inplace=True)\n",
    "stem=stem[['Metric','GEM']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating PPV and NPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_calc = out.fillna(0)\n",
    "prec_calc.reset_index(inplace=True)\n",
    "\n",
    "prec_calc.loc[prec_calc['GEM result']=='+ Numer','PPV']=prec_calc['+ Numer'].divide((prec_calc['+ Numer']+ prec_calc['- Not Numer']), axis=0, fill_value=0)#.apply(lambda x: 100*np.round(x,3))\n",
    "prec_calc.loc[prec_calc['GEM result']=='- Not Numer','NPV']=prec_calc['- Not Numer'].divide((prec_calc['+ Numer']+ prec_calc['- Not Numer']), axis=0, fill_value=0)\n",
    "prec_calcinter=prec_calc[['Metric','GEM','PPV']].dropna(subset=['PPV']).merge(prec_calc[['Metric','GEM','NPV']].dropna(subset=['NPV']), on=['Metric','GEM'], how='left')\n",
    "prec_calcinter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Rand Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_calc = unifiedcompared.pivot_table('Agree', ['Metric','GEM'], 'Agreement_eval')\n",
    "acc_calc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_calc.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_calc['True Cases']=acc_calc['TNot_Numer']+acc_calc['TNumer']\n",
    "acc_calc['False Cases']=acc_calc['FNot_Numer']+acc_calc['FNumer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_calc['Rand Accuracy']=acc_calc['True Cases'].divide((acc_calc['True Cases']+acc_calc['False Cases']), axis=0, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring Sens, Spec, PPV, NPV, and Rand Accuracy into single DataFrame \n",
    "w/ subsequent calculated LR, and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingassess = stem.merge(sens_spec[['Metric','GEM','Sensitivity']].dropna(subset=['Sensitivity']), on=['Metric','GEM'], how='left')\n",
    "workingassess = workingassess.merge(sens_spec[['Metric','GEM', 'Specificity']].dropna(subset=['Specificity']), on=['Metric','GEM'], how='left')\n",
    "workingassess = workingassess.merge(prec_calcinter, on=['Metric','GEM'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingassess=workingassess.merge(acc_calc[['Metric','GEM','Rand Accuracy','True Cases','False Cases','TNumer','FNumer','TNot_Numer','FNot_Numer']], on=['Metric','GEM'], how='outer')\n",
    "workingassess=workingassess.fillna(0)\n",
    "workingassess['LR +']=workingassess['Sensitivity'].divide((1-workingassess['Specificity']), axis=0, fill_value=0)\n",
    "workingassess['LR -']=(1-workingassess['Sensitivity']).divide((workingassess['Specificity']), axis=0, fill_value=0)\n",
    "print('PROCESSING UPDATE: \\nOutput of Assessment Metrics stored in \\'workingassess\\' dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingassess.to_csv('./scrapdir/'+coded_date+'/workingassess_'+coded_date+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
